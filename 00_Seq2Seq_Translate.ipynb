{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en\n",
    "# !python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1e6fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 11:08:00.555139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-29 11:08:00.710187: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-29 11:08:01.625032: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-09-29 11:08:01.625126: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-09-29 11:08:01.625134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/lib/python3.8/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n",
      "/opt/conda/lib/python3.8/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths   = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths  = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ac659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'can', 'now', 'install', 'TorchText', 'using', 'pip', '!']\n",
      "Counter({'a': 2, 'b': 1})\n"
     ]
    }
   ],
   "source": [
    "# Visualize torchtex.utils.get_tokenize\n",
    "token = de_tokenizer(\"You can now install TorchText using pip!\")\n",
    "print(token)\n",
    "# Visualize Counter\n",
    "counter = Counter(['a', 'a', 'b'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0380013",
   "metadata": {},
   "source": [
    "### PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcae598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(file_path, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(file_path, encoding=\"utf8\") as f: \n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    return Vocab(counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0fd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15285cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
      "Counter({'a': 2, 'b': 1, 'Two': 1, 'young': 1, ',': 1, 'White': 1, 'males': 1, 'are': 1, 'outside': 1, 'near': 1, 'many': 1, 'bushes': 1, '.': 1})\n",
      "['Several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']\n",
      "Counter({'a': 3, 'are': 2, '.': 2, 'b': 1, 'Two': 1, 'young': 1, ',': 1, 'White': 1, 'males': 1, 'outside': 1, 'near': 1, 'many': 1, 'bushes': 1, 'Several': 1, 'men': 1, 'in': 1, 'hard': 1, 'hats': 1, 'operating': 1, 'giant': 1, 'pulley': 1, 'system': 1})\n"
     ]
    }
   ],
   "source": [
    "# Visualizing tonize in biuld_vocab\n",
    "data = [\n",
    "    'Two young, White males are outside near many bushes.' \n",
    "    ,'Several men in hard hats are operating a giant pulley system.'\n",
    "    ,'A little girl climbing into a wooden playhouse.'\n",
    "    ,'A man in a blue shirt is standing on a ladder cleaning a window.'\n",
    "    ,'Two men are at the stove preparing food.'\n",
    "    ,'A man in green holds a guitar while the other man observes his shirt.'\n",
    "    ,'A man is smiling at a stuffed lion'\n",
    "    ,'A trendy girl talking on her cellphone while gliding slowly down the street.'\n",
    "    ,'A woman with a large purse is walking by a gate.'\n",
    "]\n",
    "\n",
    "for string_ in data[:2]:\n",
    "    print(en_tokenizer(string_))\n",
    "    counter.update(en_tokenizer(string_))\n",
    "    print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1b303b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths): \n",
    "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    data = [] \n",
    "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "        de_tensor = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
    "        en_tensor = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
    "        \n",
    "        data.append((de_tensor, en_tensor))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af1cdbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(train_filepaths)\n",
    "val_data   = data_process(val_filepaths)\n",
    "test_data  = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7bec1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two young, White males are outside near many bushes.\\n', 'Several men in hard hats are operating a giant pulley system.\\n', 'A little girl climbing into a wooden playhouse.\\n', 'A man in a blue shirt is standing on a ladder cleaning a window.\\n', 'Two men are at the stove preparing food.\\n', 'A man in green holds a guitar while the other man observes his shirt.\\n', 'A man is smiling at a stuffed lion\\n', 'A trendy girl talking on her cellphone while gliding slowly down the street.\\n', 'A woman with a large purse is walking by a gate.\\n', 'Boys dancing on poles in the middle of the night.\\n'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize in data process function \n",
    "raw_en_iter = iter(io.open(train_filepaths[1], encoding=\"utf8\"))\n",
    "print(list(raw_en_iter)[:10], \"\\n\")\n",
    "raw_en_iter = list(raw_en_iter)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df040251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17458,  7359, 14847,   302,  7378, 31707,     8,  2323,    45,   615,\n",
       "         9923,    51,  6871, 31707,     9,   248,  3963,  6175,     7,  7378,\n",
       "           71, 14847,  9923,   460, 27623, 29000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d63a4",
   "metadata": {},
   "source": [
    "### DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3a48740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "PAD = de_vocab['<pad>']\n",
    "BOS = de_vocab['<bos>']\n",
    "EOS = de_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "004b12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch): \n",
    "    de_batch, en_batch = [], [] \n",
    "    for (de_item, en_item) in data_batch: \n",
    "        de_batch.append(torch.cat([torch.tensor([BOS]), de_item, torch.tensor([EOS])], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS]), de_item, torch.tensor([EOS])], dim=0))\n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD)\n",
    "    return de_batch, en_batch\n",
    "        \n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "test_iter  = DataLoader(test_data,  batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "val_iter   = DataLoader(val_data,   batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2763363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de tensor([ 3177,   498,   143,  1662,   490,  3079,   466, 11832,  4501,   383,\n",
      "            2,     5, 28809, 29000])\n",
      "en tensor([ 3116,  2074,  3963,    23,    39,  3716,   870,   569,   120,    19,\n",
      "        27623, 29000])\n",
      "de tensor([  502,  1662,  8838,    33,     9,  4947,     1, 28809, 29000])\n",
      "en tensor([  280,  1612, 14847,   120,   146,  3716,    22, 31707,    43,     3,\n",
      "            6, 27623, 29000])\n",
      "de tensor([13904,   754,  2121,   171, 11832,  4947,     2,   906,    61, 28809,\n",
      "        29000])\n",
      "en tensor([17458,   843,  1667,   193,   711, 31707,   174,     3, 27623, 29000])\n",
      "de tensor([13904,  7805, 11832, 13697,   992,  1202,  1778,  8686,  6750,    56,\n",
      "         8925,    43,  4947,   148, 28809, 29000])\n",
      "en tensor([17458,  7359, 14847, 31707,  1871,  2323,  7524,  1612,  8019, 31707,\n",
      "           60,    60, 31707,   183, 27623, 29000])\n",
      "de tensor([ 3177,  1662,   926,   902,    17,  8925,    78,   232,  1905, 28809,\n",
      "        29000])\n",
      "en tensor([ 3116,  1612,  3716,  2908,  9923,    23,   104,   352, 27623, 29000])\n",
      "de tensor([13904,  7805, 11832,    46,  1310,  3976,   303,  8938,  1386,  4501,\n",
      "          411,  7805,   290,  1202,    12, 28809, 29000])\n",
      "en tensor([17458,  7359, 14847,   920,   322, 31707,   365,  1932,  9923,   660,\n",
      "         7359,    13,  1969,  2323, 27623, 29000])\n",
      "de tensor([13904,  7805,   234,  3479,     3,     3,  3044, 28809, 29000])\n",
      "en tensor([17458,  7359,  7524,   344,  2908, 31707,    32,     7, 29000])\n"
     ]
    }
   ],
   "source": [
    "# Visualize in one batch_size\n",
    "\n",
    "batch = train_data[:7]\n",
    "de_batch, en_batch = [], [] \n",
    "for (de_item, en_item) in batch: \n",
    "    print(\"de\",de_item)\n",
    "    print(\"en\",en_item)\n",
    "    \n",
    "    de_batch.append(torch.cat([torch.tensor([BOS]), de_item, torch.tensor([EOS])], dim=0))\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS]), en_item, torch.tensor([EOS])], dim=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35d35421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0])]\n",
      "[tensor([13904,  7805,   234,  3479,     3,     3,  3044, 28809, 29000])]\n",
      "[tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "print([torch.tensor([BOS])])\n",
    "print([de_item])\n",
    "print([torch.tensor([EOS])])\n",
    "\n",
    "# All will concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af292d9",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0278be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from typing import Tuple \n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int, \n",
    "        emb_dim: int, \n",
    "        enc_hid_dim: int, \n",
    "        dec_hid_dim: int, \n",
    "        dropout: float\n",
    "    ): \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.emb_dim = emb_dim \n",
    "        self.enc_hid_dim = enc_hid_dim \n",
    "        self.dec_hid_dim = dec_hid_dim \n",
    "        self.dropout = dropout \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor\n",
    "    ) -> Tuple[Tensor]:\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],\n",
    "                                               hidden[-1,:,:]),\n",
    "                                               dim = 1)))\n",
    "        return outputs, hidden\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module): \n",
    "    def __init__(\n",
    "        self,\n",
    "        enc_hid_dim:int,\n",
    "        dec_hid_dim:int,\n",
    "        attn_dim:int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim \n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        secoder_hidden:Tensor,\n",
    "        encoder_outputs:Tensor\n",
    "    ) -> Tensor: \n",
    "        sec_len = encoder_outputs.shape[0]\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        energy = torch.tanh(self.attn(torch.cat(\n",
    "            repeated_decoder_hidden, \n",
    "            encoder_outputs,\n",
    "            dim =2\n",
    "        )))\n",
    "        attention = torch.sum(encergy, dim=2)\n",
    "        return f.softmax(attention, dim=1)\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim:int,\n",
    "        emb_dim: int,\n",
    "        enc_hid_dim: int,\n",
    "        dec_hid_dim: int,\n",
    "        dropout: int, \n",
    "        attention: nn.Module\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim \n",
    "        self.enc_hid_dim = enc_hid_dim \n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim \n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim) \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim) \n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _weighted_encoder_rep(\n",
    "        self,\n",
    "        decoder_hidden: Tensor,\n",
    "        encoder_outputs: Tensor\n",
    "    ) -> Tensor: \n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "        a = a.unsqueze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "        weighted_encoder_erp = weighted_encoder_rep.permute(1, 0, 2)\n",
    "        return weighted_encoder_rep\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input: Tensor, \n",
    "        decoder_hidden: Tensor, \n",
    "        encoder_outputs: Tensor\n",
    "   ) -> Tuple[Tensor]: \n",
    "        \n",
    "        input_ = input_.unsqueenze(0)\n",
    "        embedded = self.dropout(self.embedding(input_))\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                         encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weightted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "        output = self.out(torch.cat((\n",
    "            output,\n",
    "            weighted_encoder_rep,\n",
    "            embedded),\n",
    "            dim = 1 \n",
    "        ))\n",
    "        return output, decoder_hidden.squeeze(0)\n",
    "        \n",
    "        \n",
    "class Seq2Seq(nn.Module): \n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        src:Tensor,\n",
    "        trg:Tensor,\n",
    "        teach_forcing_ratio: float = 0.5\n",
    "    ) -> Tensor:\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden =self.encoder(src)\n",
    "        output = trg[0, :]\n",
    "        \n",
    "        for t in range(1, max_len): \n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7654ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(de_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "encoder_embbeding_dim = 32\n",
    "decoder_embbeding_dim = 32 \n",
    "encoder_hidden_dim = 64\n",
    "decoder_hidden_dim = 64 \n",
    "attention_dim = 8 \n",
    "encoder_dropout = 0.5 \n",
    "decoder_dropout = 0.5 \n",
    "\n",
    "encoder = Encoder(\n",
    "                  input_dim,\n",
    "                  encoder_embbeding_dim,\n",
    "                  encoder_hidden_dim,\n",
    "                  decoder_hidden_dim,\n",
    "                  encoder_dropout\n",
    "                 )\n",
    "\n",
    "attention = Attention(\n",
    "                  encoder_hidden_dim,\n",
    "                  decoder_hidden_dim,\n",
    "                  attention_dim\n",
    "                 )\n",
    "\n",
    "decoder = Decoder(\n",
    "                  output_dim, \n",
    "                  decoder_embbeding_dim,\n",
    "                  encoder_hidden_dim,\n",
    "                  decoder_hidden_dim,\n",
    "                  decoder_dropout,\n",
    "                  attention\n",
    "                 )\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "676846dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,489,914 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "# PAD = en_vocab.get_itos['<pad>']\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: torch.utils.data.DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: torch.utils.data.DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73890c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52706437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
